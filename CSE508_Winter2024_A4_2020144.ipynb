{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the review csv in dataset folder\n",
    "import pandas as pd\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "# Initialize a GPT-2 tokenizer and model from Hugging Face.\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer, DataCollatorForSeq2Seq\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = pd.read_csv('dataset/Reviews.csv')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that are not needed\n",
    "review_df = review_df.drop(columns=['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Time', 'Score'])\n",
    "review_df = review_df.dropna()\n",
    "# Choose 10k rows randomly\n",
    "\n",
    "# drop rows with text concatenated with summary greater than 1024\n",
    "review_df = review_df.drop(review_df[review_df['Text'].str.len() + review_df['Summary'].str.len() > 800].index)\n",
    "\n",
    "# iterate over dataframe\n",
    "for index, row in review_df.iterrows():\n",
    "    # if length of review_df['Text'] + ' TL;DR: ' + review_df['Summary'] is greater than 1024\n",
    "    if len(row['Text']) +  len(' TL;DR: ') + len(row['Summary']) > 1024:\n",
    "        # then truncate the review_df['Text'] to 1024 - len(' TL;DR: ') - len(row['Summary'])\n",
    "        review_df.at[index, 'Text'] = row['Text'][:1024 - len(' TL;DR: ') - len(row['Summary'])]\n",
    "        # then concatenate the truncated review_df['Text'] with ' TL;DR: ' and review_df['Summary']\n",
    "        review_df.at[index, 'concat'] = review_df.at[index, 'Text'] + ' TL;DR: ' + review_df.at[index, 'Summary']\n",
    "\n",
    "    else:\n",
    "        # else concatenate the review_df['Text'] with ' TL;DR: ' and review_df['Summary']\n",
    "        review_df.at[index, 'concat'] = row['Text'] + ' TL;DR: ' + row['Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = review_df.sample(n=10000)\n",
    "# test train split\n",
    "train_df, test_df = train_test_split(review_df, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = test_df['concat'].str.len().max()\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50258, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50258, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.data = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            review = (row['concat'])\n",
    "            tokenized_text = tokenizer.encode(review, add_special_tokens=True , padding = True, max_length=MAX_LENGTH+1)\n",
    "            tokenized_text.append(tokenizer.eos_token_id)\n",
    "            self.data.append(tokenized_text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jyoti/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SummaryDataset(tokenizer=tokenizer, max_length=MAX_LENGTH ,df=train_df)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22500' max='22500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22500/22500 26:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.539100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.523400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.435400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.375500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.392800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.375100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.358300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.323000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.350900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.269600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.322200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.306600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.283600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.254400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.887400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.911100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.918700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.858300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.948600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.907200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.916100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.889000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.912600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.957000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.885700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>2.911000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.909100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>2.903500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.914000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>2.644500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.654900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>2.650900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>2.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>2.626100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>2.611800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>2.622100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>2.660800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>2.649800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.660200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>2.650700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>2.645500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>2.661600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>2.652600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>2.638000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        per_device_train_batch_size=1,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=5.6e-5,            # learning rate\n",
    "        # logging_dir='./logs',            # directory for storing logs\n",
    "        save_strategy=\"no\",\n",
    "        use_cpu=False,\n",
    "        fp16=True if device == \"cuda\" else False  # Enable mixed precision if using GPU\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 50, 'do_sample': True}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('model/tokenizer_config.json',\n",
       " 'model/special_tokens_map.json',\n",
       " 'model/vocab.json',\n",
       " 'model/merges.txt',\n",
       " 'model/added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('model')\n",
    "tokenizer.save_pretrained('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('model')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'r': 0.07823015873015873, 'p': 0.030237229273993974, 'f': 0.041010727852751866}, 'rouge-2': {'r': 0.005833333333333333, 'p': 0.0024431818181818183, 'f': 0.00335294109879585}, 'rouge-l': {'r': 0.0768015873015873, 'p': 0.029126118162882866, 'f': 0.039760727852751865}}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "summarizer = pipeline(\"text-generation\", model=model , tokenizer=tokenizer)\n",
    "\n",
    "prediction =[]\n",
    "actual =[]\n",
    "\n",
    "for col , row in test_df[:100].iterrows():\n",
    "    ARTICLE = row['Text']\n",
    "    SUMMARY = row['Summary']\n",
    "    ARTICLE = ARTICLE + \" TL;DR:\"\n",
    "\n",
    "    ans = summarizer(ARTICLE, min_new_tokens=20, max_new_tokens=80, top_k = 2 )\n",
    "\n",
    "    prediction.append(ans[0]['generated_text'][len(ARTICLE):])\n",
    "    actual.append(SUMMARY)\n",
    "\n",
    "    # print(\"Article: \", ARTICLE)\n",
    "    # print(\"Actual Summary: \", SUMMARY)\n",
    "    # print(\"Predicted Summary: \", ans[0]['generated_text'][len(ARTICLE):])\n",
    "\n",
    "\n",
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(prediction, actual, avg=True)\n",
    "print(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article:  We love these organic corn chips. They have an excellent crunch big chips great for dipping. Try with  Salsa, Mild, Organic, 17.5 oz. or homemade salsa. We have these on suscribe and save so we never run out. Very delicious corn taste similar to fritos. TL;DR:\n",
      "Actual Summary:  Most Excellent Organic yellow corn chips. Great with homemade salsa.\n",
      "Predicted Summary:   Delicious! Great taste and great price! Great for dipping. Great for dipping in salsa. Great for dipping in salsa.\n"
     ]
    }
   ],
   "source": [
    "ARTICLE = test_df['Text'].iloc[1]\n",
    "SUMMARY = test_df['Summary'].iloc[1]\n",
    "ARTICLE = ARTICLE + \" TL;DR:\"\n",
    "\n",
    "ans = summarizer(ARTICLE, min_new_tokens=20, max_new_tokens=100, top_k = 2 )\n",
    "\n",
    "prediction = ans[0]['generated_text'][len(ARTICLE):]\n",
    "actual = SUMMARY\n",
    "\n",
    "\n",
    "print(\"Article: \", ARTICLE)\n",
    "print(\"Actual Summary: \", SUMMARY)\n",
    "print(\"Predicted Summary: \", prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
